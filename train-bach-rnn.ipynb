{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 21:03:20.366037: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-17 21:03:20.377218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731906200.390883   91446 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731906200.394755   91446 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 21:03:20.408505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From /tmp/ipykernel_91446/3035087168.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731906201.656143   91446 gpu_device.cc:2022] Created device /device:GPU:0 with 4280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Physical GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Is GPU available:\", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 229 chorales from train directory.\n",
      "Loaded 76 chorales from valid directory.\n",
      "Loaded 77 chorales from test directory.\n"
     ]
    }
   ],
   "source": [
    "def load_chorales(data_dir=\"jsb_chorales 2\"):\n",
    "\n",
    "    dataset = {}\n",
    "    for subset in ['train', 'valid', 'test']:\n",
    "        subset_path = os.path.join(data_dir, subset)\n",
    "        chorales = []\n",
    "        for file in sorted(os.listdir(subset_path)):\n",
    "            if file.endswith(\".csv\"):\n",
    "                filepath = os.path.join(subset_path, file)\n",
    "                chorale = np.loadtxt(filepath, delimiter=\",\", skiprows=1, dtype=int)\n",
    "                chorales.append(chorale)\n",
    "        dataset[subset] = chorales\n",
    "        print(f\"Loaded {len(chorales)} chorales from {subset} directory.\")\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "chorale_data = load_chorales()\n",
    "train_chorales = chorale_data['train']\n",
    "valid_chorales = chorale_data['valid']\n",
    "test_chorales = chorale_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the max value for normalizing\n",
    "max_value = max([chorale.max() for chorale in train_chorales])\n",
    "max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "def create_dataset(chorales, seq_length):\n",
    "    X, y = [], []\n",
    "    for chorale in chorales:\n",
    "        for i in range(len(chorale) - seq_length):\n",
    "            X.append(chorale[i:i + seq_length])\n",
    "            y.append(chorale[i + seq_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate combinations of models to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1: (32, 64, 16, 10)\n",
      "Combination 2: (32, 64, 16, 20)\n",
      "Combination 3: (32, 64, 32, 10)\n",
      "Combination 4: (32, 64, 32, 20)\n",
      "Combination 5: (32, 128, 16, 10)\n",
      "Combination 6: (32, 128, 16, 20)\n",
      "Combination 7: (32, 128, 32, 10)\n",
      "Combination 8: (32, 128, 32, 20)\n",
      "Combination 9: (64, 64, 16, 10)\n",
      "Combination 10: (64, 64, 16, 20)\n",
      "Combination 11: (64, 64, 32, 10)\n",
      "Combination 12: (64, 64, 32, 20)\n",
      "Combination 13: (64, 128, 16, 10)\n",
      "Combination 14: (64, 128, 16, 20)\n",
      "Combination 15: (64, 128, 32, 10)\n",
      "Combination 16: (64, 128, 32, 20)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {\n",
    "    'rnn_units': [32, 64],\n",
    "    'dense_units': [64, 128],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "\n",
    "# Print all combinations\n",
    "for i, params in enumerate(param_combinations, 1):\n",
    "    print(f\"Combination {i}: {params}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_length, input_dim, rnn_units, dense_units):\n",
    "\n",
    "    inputs = layers.Input(shape=(seq_length, input_dim))\n",
    "    x = layers.SimpleRNN(rnn_units, activation=\"tanh\", return_sequences=False)(inputs)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(input_dim, activation=\"linear\")(x)  # Linear activation for regression\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply cross validation to all models to find the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameter combination 1/16: (32, 64, 16, 10)\n",
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731906201.995130   91446 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731906203.454822   91542 service.cc:148] XLA service 0x71280c0051c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731906203.454838   91542 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-17 21:03:23.475005: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731906203.545366   91542 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-17 21:03:23.575019: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1731906204.030390   91542 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 1 average validation loss: 0.0007 cross val losses: [0.0006388672045432031, 0.0006776363006792963, 0.0008181043085642159, 0.0004806887009181082, 0.0007238325197249651]\n",
      "Testing parameter combination 2/16: (32, 64, 16, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 2 average validation loss: 0.0007 cross val losses: [0.0005793540622107685, 0.0006682360544800758, 0.0009619655320420861, 0.0005251574912108481, 0.0008620915468782187]\n",
      "Testing parameter combination 3/16: (32, 64, 32, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 3 average validation loss: 0.0008 cross val losses: [0.0009896669071167707, 0.0007530309958383441, 0.0008342136279679835, 0.0005220957682467997, 0.0006884207832626998]\n",
      "Testing parameter combination 4/16: (32, 64, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 4 average validation loss: 0.0007 cross val losses: [0.0006202165386639535, 0.0006879002321511507, 0.0009361971169710159, 0.0004431807028595358, 0.0006187458639033139]\n",
      "Testing parameter combination 5/16: (32, 128, 16, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 5 average validation loss: 0.0007 cross val losses: [0.0006270285812206566, 0.00064588658278808, 0.0009355357615277171, 0.000490466714836657, 0.0006288528093136847]\n",
      "Testing parameter combination 6/16: (32, 128, 16, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 6 average validation loss: 0.0007 cross val losses: [0.0006111420225352049, 0.0006314593483693898, 0.0009393311920575798, 0.0005180728621780872, 0.0007809466333128512]\n",
      "Testing parameter combination 7/16: (32, 128, 32, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 7 average validation loss: 0.0007 cross val losses: [0.0006652004085481167, 0.000770686543546617, 0.0009759730892255902, 0.0005330726271495223, 0.0006827309844084084]\n",
      "Testing parameter combination 8/16: (32, 128, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 8 average validation loss: 0.0007 cross val losses: [0.0007961784140206873, 0.0007073585875332355, 0.0008795122266747057, 0.0004216113011352718, 0.0006003976450301707]\n",
      "Testing parameter combination 9/16: (64, 64, 16, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 9 average validation loss: 0.0007 cross val losses: [0.0006159890908747911, 0.0007826754590496421, 0.0008549343910999596, 0.00047279425780288875, 0.0006413814844563603]\n",
      "Testing parameter combination 10/16: (64, 64, 16, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 10 average validation loss: 0.0007 cross val losses: [0.0006128543755039573, 0.0007045706734061241, 0.0009018869022838771, 0.0004615466750692576, 0.0007580743986181915]\n",
      "Testing parameter combination 11/16: (64, 64, 32, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 11 average validation loss: 0.0007 cross val losses: [0.0006338564562611282, 0.0008160798461176455, 0.0009225952671840787, 0.0004334028053563088, 0.0006041333544999361]\n",
      "Testing parameter combination 12/16: (64, 64, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 12 average validation loss: 0.0007 cross val losses: [0.0006958682206459343, 0.0008405558182857931, 0.0008652462856844068, 0.0004571729514282197, 0.0006309722666628659]\n",
      "Testing parameter combination 13/16: (64, 128, 16, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 13 average validation loss: 0.0009 cross val losses: [0.0006180452764965594, 0.000712433597072959, 0.0010835037101060152, 0.001242317259311676, 0.000618040794506669]\n",
      "Testing parameter combination 14/16: (64, 128, 16, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 14 average validation loss: 0.0007 cross val losses: [0.0007255192613229156, 0.0007064369274303317, 0.0009306988213211298, 0.0004986269632354379, 0.0006405655294656754]\n",
      "Testing parameter combination 15/16: (64, 128, 32, 10)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 15 average validation loss: 0.0007 cross val losses: [0.0006275721825659275, 0.0006924046901986003, 0.0008707848028279841, 0.0004990303423255682, 0.0006100333412177861]\n",
      "Testing parameter combination 16/16: (64, 128, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 16 average validation loss: 0.0007 cross val losses: [0.0006536251748912036, 0.0007042805664241314, 0.0010356171987950802, 0.00046770821791142225, 0.0005848209257237613]\n",
      "Best parameters: (64, 128, 32, 10) with validation loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "SEQ_LENGTH = 32  # Length of input sequences\n",
    "\n",
    "X_train, y_train = create_dataset(train_chorales, SEQ_LENGTH)\n",
    "X_valid, y_valid = create_dataset(valid_chorales, SEQ_LENGTH)\n",
    "X_test, y_test = create_dataset(test_chorales, SEQ_LENGTH)\n",
    "\n",
    "# Normalize and reshape data\n",
    "X_train, X_valid, X_test = X_train / max_value, X_valid / max_value, X_test / max_value\n",
    "y_train, y_valid, y_test = y_train / max_value, y_valid / max_value, y_test / max_value\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "fold_results = []\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for idx, params in enumerate(param_combinations):\n",
    "    print(f\"Testing parameter combination {idx + 1}/{len(param_combinations)}: {params}\")\n",
    "    \n",
    "    # Extract hyperparameters for this combination\n",
    "    rnn_units, dense_units, batch_size, epochs = params\n",
    "\n",
    "    fold_losses = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"Training fold {fold + 1}/{kf.n_splits}...\")\n",
    "        \n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Build the model\n",
    "\n",
    "        model = build_model(SEQ_LENGTH, X_train.shape[-1], rnn_units, dense_units)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            callbacks=[early_stopping, tensorboard_callback],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Save fold loss\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        fold_losses.append(final_val_loss)\n",
    "    \n",
    "    # Calculate average validation loss across folds\n",
    "    avg_val_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Parameter combination {idx + 1} average validation loss: {avg_val_loss:.4f} cross val losses: {fold_losses}\")\n",
    "    \n",
    "    # Update best parameters if current loss is lower\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters: {best_params} with validation loss: {best_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0162 - mae: 0.0431 - val_loss: 0.0018 - val_mae: 0.0164\n",
      "Epoch 2/10\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 7.7283e-04 - mae: 0.0160 - val_loss: 0.0017 - val_mae: 0.0179\n",
      "Epoch 3/10\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 7.3333e-04 - mae: 0.0145 - val_loss: 0.0016 - val_mae: 0.0151\n",
      "Epoch 4/10\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 6.7991e-04 - mae: 0.0142 - val_loss: 0.0020 - val_mae: 0.0217\n",
      "Epoch 5/10\n",
      "\u001b[1m1497/1497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 6.2001e-04 - mae: 0.0136 - val_loss: 0.0016 - val_mae: 0.0173\n"
     ]
    }
   ],
   "source": [
    "# Unpack best parameters\n",
    "rnn_units, dense_units, batch_size, epochs= best_params\n",
    "\n",
    "# Build the final model\n",
    "final_model = build_model(SEQ_LENGTH, X_train.shape[-1], rnn_units, dense_units)\n",
    "\n",
    "# Train the model\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping, tensorboard_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - mae: 0.0155\n",
      "Test loss: 0.0013, Test MAE: 0.0155\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = final_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Generated music shape: (48, 4)\n",
      "[[65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [72 60 55 52]\n",
      " [72 60 55 52]\n",
      " [70 60 55 52]\n",
      " [70 60 55 52]\n",
      " [69 60 53 53]\n",
      " [69 60 53 53]\n",
      " [67 60 55 52]\n",
      " [67 60 55 52]\n",
      " [65 62 57 50]\n",
      " [65 62 57 50]\n",
      " [65 64 58 50]\n",
      " [65 64 58 50]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [74 65 60 46]\n",
      " [74 65 60 46]\n",
      " [74 65 58 48]\n",
      " [74 65 57 48]\n",
      " [74 65 58 50]\n",
      " [74 65 58 50]\n",
      " [74 65 58 52]\n",
      " [74 65 58 52]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]]\n",
      "\tVVVVVV\n",
      "[[72 65 56 52]\n",
      " [72 65 57 52]\n",
      " [72 65 57 51]\n",
      " [72 66 57 51]\n",
      " [72 66 57 51]\n",
      " [73 66 58 51]\n",
      " [73 66 58 51]\n",
      " [73 66 58 51]\n",
      " [73 66 58 51]\n",
      " [73 67 58 51]\n",
      " [73 67 58 51]\n",
      " [74 67 58 51]\n",
      " [74 67 59 51]\n",
      " [74 67 59 51]\n",
      " [74 67 59 51]\n",
      " [74 67 59 51]]\n"
     ]
    }
   ],
   "source": [
    "# Generate Bach-like music using the trained model\n",
    "def generate_music(model, seed_sequence, num_steps=16):\n",
    "    generated = seed_sequence.copy()\n",
    "    for _ in range(num_steps):\n",
    "        # Prepare input for the model\n",
    "        input_seq = generated[-SEQ_LENGTH:].reshape(1, SEQ_LENGTH, -1)\n",
    "        # Predict the next step\n",
    "        next_step = model.predict(input_seq)\n",
    "        generated = np.vstack((generated, next_step))  # Append the new step\n",
    "    return generated\n",
    "\n",
    "# Use the first test sequence as the seed\n",
    "seed_sequence = X_test[0]\n",
    "steps = 16\n",
    "generated_music = generate_music(final_model, seed_sequence, num_steps=steps)\n",
    "\n",
    "# Denormalize and convert to integers\n",
    "generated_music = (generated_music * max_value).astype(int)\n",
    "initial_music = (seed_sequence * max_value).astype(int)\n",
    "print(\"Generated music shape:\", generated_music.shape)\n",
    "print(initial_music)\n",
    "print(\"\\tVVVVVV\")\n",
    "print(generated_music[-steps:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
