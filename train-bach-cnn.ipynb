{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:43:07.021241: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-17 19:43:07.245736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731901387.325152    6409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731901387.348412    6409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 19:43:07.552341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6409/3035087168.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731901390.845886    6409 gpu_device.cc:2022] Created device /device:GPU:0 with 4280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Physical GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Is GPU available:\", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 229 chorales from train directory.\n",
      "Loaded 76 chorales from valid directory.\n",
      "Loaded 77 chorales from test directory.\n"
     ]
    }
   ],
   "source": [
    "def load_chorales(data_dir=\"jsb_chorales 2\"):\n",
    "\n",
    "    dataset = {}\n",
    "    for subset in ['train', 'valid', 'test']:\n",
    "        subset_path = os.path.join(data_dir, subset)\n",
    "        chorales = []\n",
    "        for file in sorted(os.listdir(subset_path)):\n",
    "            if file.endswith(\".csv\"):\n",
    "                filepath = os.path.join(subset_path, file)\n",
    "                chorale = np.loadtxt(filepath, delimiter=\",\", skiprows=1, dtype=int)\n",
    "                chorales.append(chorale)\n",
    "        dataset[subset] = chorales\n",
    "        print(f\"Loaded {len(chorales)} chorales from {subset} directory.\")\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "chorale_data = load_chorales()\n",
    "train_chorales = chorale_data['train']\n",
    "valid_chorales = chorale_data['valid']\n",
    "test_chorales = chorale_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the max value for normalizing\n",
    "max_value = max([chorale.max() for chorale in train_chorales])\n",
    "max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "def create_dataset(chorales, seq_length):\n",
    "    X, y = [], []\n",
    "    for chorale in chorales:\n",
    "        for i in range(len(chorale) - seq_length):\n",
    "            X.append(chorale[i:i + seq_length])\n",
    "            y.append(chorale[i + seq_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate combinations of models to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {\n",
    "    'conv1_filters': [32, 64],\n",
    "    'conv2_filters': [64, 128],\n",
    "    'dense_units': [64, 128],\n",
    "    'kernel_size': [3, 5],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_length, input_dim, conv1_filters, conv2_filters, dense_units, kernel_size):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(conv1_filters, kernel_size=kernel_size, activation=\"relu\", input_shape=(seq_length, input_dim)),\n",
    "        layers.Conv1D(conv2_filters, kernel_size=kernel_size, activation=\"relu\"),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(dense_units, activation=\"relu\"),\n",
    "        layers.Dense(input_dim, activation=\"linear\")  # Predict the next time step\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply cross validation to all models to find the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameter combination 1/32: (32, 64, 64, 3, 32, 20)\n",
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1731901391.366270    6409 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4280 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731901392.882342    6830 service.cc:148] XLA service 0x73c80c009140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731901392.882522    6830 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-17 19:43:12.908991: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731901393.000487    6830 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-17 19:43:13.034886: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1731901394.268360    6830 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 1 average validation loss: 0.0020 cross val losses: [0.0018727758433669806, 0.001837266725488007, 0.0026115223299711943, 0.002067663474008441, 0.0017300293548032641]\n",
      "Testing parameter combination 2/32: (32, 64, 64, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 2 average validation loss: 0.0019 cross val losses: [0.002026095986366272, 0.001911328174173832, 0.002027608687058091, 0.0017630556831136346, 0.0018476444529369473]\n",
      "Testing parameter combination 3/32: (32, 64, 64, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 3 average validation loss: 0.0019 cross val losses: [0.0017857810016721487, 0.0016570644220337272, 0.002118657110258937, 0.002208494348451495, 0.0019713337533175945]\n",
      "Testing parameter combination 4/32: (32, 64, 64, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 4 average validation loss: 0.0020 cross val losses: [0.0019442072371020913, 0.0018569213571026921, 0.0021036467514932156, 0.0021626183297485113, 0.001744734006933868]\n",
      "Testing parameter combination 5/32: (32, 64, 128, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 5 average validation loss: 0.0021 cross val losses: [0.001820137957111001, 0.001925312913954258, 0.0020203543826937675, 0.0023847369011491537, 0.00220128009095788]\n",
      "Testing parameter combination 6/32: (32, 64, 128, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 6 average validation loss: 0.0019 cross val losses: [0.0019650899339467287, 0.0020708448719233274, 0.00194319779984653, 0.0018649998819455504, 0.0018966802163049579]\n",
      "Testing parameter combination 7/32: (32, 64, 128, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 7 average validation loss: 0.0018 cross val losses: [0.0017436171183362603, 0.001968419412150979, 0.001787812216207385, 0.0018942567985504866, 0.0017846169648692012]\n",
      "Testing parameter combination 8/32: (32, 64, 128, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 8 average validation loss: 0.0019 cross val losses: [0.0019090718124061823, 0.0017463085241615772, 0.0022790865041315556, 0.0017883896362036467, 0.001948233344592154]\n",
      "Testing parameter combination 9/32: (32, 128, 64, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 9 average validation loss: 0.0019 cross val losses: [0.001814049668610096, 0.0021357848308980465, 0.001977976644411683, 0.0016093442682176828, 0.002071336144581437]\n",
      "Testing parameter combination 10/32: (32, 128, 64, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 10 average validation loss: 0.0018 cross val losses: [0.001785630127415061, 0.0018343925476074219, 0.00204310636036098, 0.0016631317557767034, 0.001718326355330646]\n",
      "Testing parameter combination 11/32: (32, 128, 64, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 11 average validation loss: 0.0018 cross val losses: [0.00189303420484066, 0.0018652251455932856, 0.0019034831784665585, 0.0017912806943058968, 0.0016322614392265677]\n",
      "Testing parameter combination 12/32: (32, 128, 64, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 12 average validation loss: 0.0018 cross val losses: [0.0017858502687886357, 0.0017605359898880124, 0.0018191797425970435, 0.0015642516082152724, 0.0018821553094312549]\n",
      "Testing parameter combination 13/32: (32, 128, 128, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 13 average validation loss: 0.0019 cross val losses: [0.0021303019020706415, 0.0019946489483118057, 0.002051457529887557, 0.001736838254146278, 0.0017972295172512531]\n",
      "Testing parameter combination 14/32: (32, 128, 128, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 14 average validation loss: 0.0019 cross val losses: [0.0018708768766373396, 0.001870345906354487, 0.0020768195390701294, 0.0021034502424299717, 0.0016579878283664584]\n",
      "Testing parameter combination 15/32: (32, 128, 128, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 15 average validation loss: 0.0018 cross val losses: [0.0016825245693325996, 0.0020204016473144293, 0.0019143630051985383, 0.001618328271433711, 0.0019369989167898893]\n",
      "Testing parameter combination 16/32: (32, 128, 128, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 16 average validation loss: 0.0018 cross val losses: [0.0020615928806364536, 0.0018225735984742641, 0.0018535470589995384, 0.001542680081911385, 0.001750025781802833]\n",
      "Testing parameter combination 17/32: (64, 64, 64, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 17 average validation loss: 0.0020 cross val losses: [0.0018908838974311948, 0.0022804460022598505, 0.0020084551069885492, 0.0017083161510527134, 0.0018645209493115544]\n",
      "Testing parameter combination 18/32: (64, 64, 64, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 18 average validation loss: 0.0019 cross val losses: [0.001831867964938283, 0.0018144447822123766, 0.00207744468934834, 0.0017667397623881698, 0.0017934577772393823]\n",
      "Testing parameter combination 19/32: (64, 64, 64, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 19 average validation loss: 0.0019 cross val losses: [0.0017102634301409125, 0.0018985550850629807, 0.0019285226007923484, 0.0019397601718083024, 0.0017787376418709755]\n",
      "Testing parameter combination 20/32: (64, 64, 64, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 20 average validation loss: 0.0020 cross val losses: [0.0021912548691034317, 0.0017373813316226006, 0.0019199126400053501, 0.0022540506906807423, 0.001758489408530295]\n",
      "Testing parameter combination 21/32: (64, 64, 128, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 21 average validation loss: 0.0019 cross val losses: [0.0019889266695827246, 0.0019514842424541712, 0.002036208985373378, 0.001754357130266726, 0.0018003983423113823]\n",
      "Testing parameter combination 22/32: (64, 64, 128, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 22 average validation loss: 0.0020 cross val losses: [0.0020025477278977633, 0.002243727445602417, 0.0021161367185413837, 0.001728680683299899, 0.0019208673620596528]\n",
      "Testing parameter combination 23/32: (64, 64, 128, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 23 average validation loss: 0.0018 cross val losses: [0.0018647735705599189, 0.0016011657426133752, 0.0023533643689006567, 0.0016531568253412843, 0.0017385309329256415]\n",
      "Testing parameter combination 24/32: (64, 64, 128, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 24 average validation loss: 0.0019 cross val losses: [0.0020537974778562784, 0.0020257600117474794, 0.002081973711028695, 0.0016705450834706426, 0.0018649310804903507]\n",
      "Testing parameter combination 25/32: (64, 128, 64, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 25 average validation loss: 0.0018 cross val losses: [0.001805002219043672, 0.0019361777231097221, 0.0019933131989091635, 0.001727711409330368, 0.001786078093573451]\n",
      "Testing parameter combination 26/32: (64, 128, 64, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 26 average validation loss: 0.0019 cross val losses: [0.0017644661711528897, 0.0019489391706883907, 0.0021119399461895227, 0.0017821689834818244, 0.0017035752534866333]\n",
      "Testing parameter combination 27/32: (64, 128, 64, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 27 average validation loss: 0.0019 cross val losses: [0.0019476067973300815, 0.001777604571543634, 0.002044458407908678, 0.0017576502868905663, 0.0017729820683598518]\n",
      "Testing parameter combination 28/32: (64, 128, 64, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 28 average validation loss: 0.0018 cross val losses: [0.0018428517505526543, 0.001742204069159925, 0.0019099938217550516, 0.0017227274365723133, 0.0017579719424247742]\n",
      "Testing parameter combination 29/32: (64, 128, 128, 3, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 29 average validation loss: 0.0019 cross val losses: [0.0019620342645794153, 0.0017640813020989299, 0.001990243326872587, 0.001816903823055327, 0.0020131736528128386]\n",
      "Testing parameter combination 30/32: (64, 128, 128, 3, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 30 average validation loss: 0.0019 cross val losses: [0.0018509781220927835, 0.002155269728973508, 0.0021820671390742064, 0.001707627554424107, 0.0018145923968404531]\n",
      "Testing parameter combination 31/32: (64, 128, 128, 5, 32, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 31 average validation loss: 0.0019 cross val losses: [0.002090159337967634, 0.001858903793618083, 0.002215780084952712, 0.0015962759498506784, 0.0016916680615395308]\n",
      "Testing parameter combination 32/32: (64, 128, 128, 5, 64, 20)\n",
      "Training fold 1/5...\n",
      "Training fold 2/5...\n",
      "Training fold 3/5...\n",
      "Training fold 4/5...\n",
      "Training fold 5/5...\n",
      "Parameter combination 32 average validation loss: 0.0020 cross val losses: [0.001794066047295928, 0.002195949899032712, 0.001881201984360814, 0.002104631857946515, 0.0017779356567189097]\n",
      "Best parameters: (32, 128, 64, 5, 64, 20) with validation loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "SEQ_LENGTH = 32  # Length of input sequences\n",
    "\n",
    "max_value = max([chorale.max() for chorale in train_chorales])\n",
    "\n",
    "X_train, y_train = create_dataset(train_chorales, SEQ_LENGTH)\n",
    "X_valid, y_valid = create_dataset(valid_chorales, SEQ_LENGTH)\n",
    "X_test, y_test = create_dataset(test_chorales, SEQ_LENGTH)\n",
    "\n",
    "# Normalize and reshape data\n",
    "X_train, X_valid, X_test = X_train / max_value, X_valid / max_value, X_test / max_value\n",
    "y_train, y_valid, y_test = y_train / max_value, y_valid / max_value, y_test / max_value\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "fold_results = []\n",
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for idx, params in enumerate(param_combinations):\n",
    "    print(f\"Testing parameter combination {idx + 1}/{len(param_combinations)}: {params}\")\n",
    "    \n",
    "    # Extract hyperparameters for this combination\n",
    "    conv1_filters, conv2_filters, dense_units, kernel_size, batch_size, epochs = params\n",
    "    \n",
    "    fold_losses = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"Training fold {fold + 1}/{kf.n_splits}...\")\n",
    "        \n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Build the model\n",
    "        model = build_model(SEQ_LENGTH, X_train.shape[-1], conv1_filters, conv2_filters, dense_units, kernel_size)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            callbacks=[early_stopping, tensorboard_callback],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Save fold loss\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        fold_losses.append(final_val_loss)\n",
    "    \n",
    "    # Calculate average validation loss across folds\n",
    "    avg_val_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Parameter combination {idx + 1} average validation loss: {avg_val_loss:.4f} cross val losses: {fold_losses}\")\n",
    "    \n",
    "    # Update best parameters if current loss is lower\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters: {best_params} with validation loss: {best_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m749/749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0260 - mae: 0.0779 - val_loss: 0.0053 - val_mae: 0.0403\n",
      "Epoch 2/20\n",
      "\u001b[1m749/749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0022 - mae: 0.0339 - val_loss: 0.0037 - val_mae: 0.0364\n",
      "Epoch 3/20\n",
      "\u001b[1m749/749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0019 - mae: 0.0320 - val_loss: 0.0031 - val_mae: 0.0337\n",
      "Epoch 4/20\n",
      "\u001b[1m749/749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0019 - mae: 0.0316 - val_loss: 0.0032 - val_mae: 0.0349\n",
      "Epoch 5/20\n",
      "\u001b[1m749/749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 0.0035 - val_mae: 0.0383\n"
     ]
    }
   ],
   "source": [
    "# Unpack best parameters\n",
    "conv1_filters, conv2_filters, dense_units, kernel_size, batch_size, epochs = best_params\n",
    "\n",
    "# Build the final model\n",
    "final_model = build_model(SEQ_LENGTH, X_train.shape[-1], conv1_filters, conv2_filters, dense_units, kernel_size)\n",
    "\n",
    "# Train the model\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping, tensorboard_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0039 - mae: 0.0379 \n",
      "Test loss: 0.0037, Test MAE: 0.0375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = final_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Generated music shape: (48, 4)\n",
      "[[65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [65 60 57 53]\n",
      " [72 60 55 52]\n",
      " [72 60 55 52]\n",
      " [70 60 55 52]\n",
      " [70 60 55 52]\n",
      " [69 60 53 53]\n",
      " [69 60 53 53]\n",
      " [67 60 55 52]\n",
      " [67 60 55 52]\n",
      " [65 62 57 50]\n",
      " [65 62 57 50]\n",
      " [65 64 58 50]\n",
      " [65 64 58 50]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [72 65 60 45]\n",
      " [74 65 60 46]\n",
      " [74 65 60 46]\n",
      " [74 65 58 48]\n",
      " [74 65 57 48]\n",
      " [74 65 58 50]\n",
      " [74 65 58 50]\n",
      " [74 65 58 52]\n",
      " [74 65 58 52]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]\n",
      " [72 65 57 53]]\n",
      "\tVVVVVV\n",
      "[[68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 58 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]\n",
      " [68 63 57 48]]\n"
     ]
    }
   ],
   "source": [
    "# Generate Bach-like music using the trained model\n",
    "def generate_music(model, seed_sequence, num_steps=16):\n",
    "    generated = seed_sequence.copy()\n",
    "    for _ in range(num_steps):\n",
    "        # Prepare input for the model\n",
    "        input_seq = generated[-SEQ_LENGTH:].reshape(1, SEQ_LENGTH, -1)\n",
    "        # Predict the next step\n",
    "        next_step = model.predict(input_seq)\n",
    "        generated = np.vstack((generated, next_step))  # Append the new step\n",
    "    return generated\n",
    "\n",
    "# Use the first test sequence as the seed\n",
    "seed_sequence = X_test[0]\n",
    "steps = 16\n",
    "generated_music = generate_music(final_model, seed_sequence, num_steps=steps)\n",
    "\n",
    "# Denormalize and convert to integers\n",
    "generated_music = (generated_music * max_value).astype(int)\n",
    "initial_music = (seed_sequence * max_value).astype(int)\n",
    "print(\"Generated music shape:\", generated_music.shape)\n",
    "print(initial_music)\n",
    "print(\"\\tVVVVVV\")\n",
    "print(generated_music[-steps:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
